\documentclass[a4paper,12pt]{article}
% \usepackage[utf8]{inputenc}
\usepackage{parskip}
% \usepackage[sorting=none]{biblatex}
% \usepackage{url}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{fancyhdr}

\usepackage{mathtools}
% \usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{arydshln}
\usepackage{tikz}
\usepackage{todonotes}

\usetikzlibrary{shapes.geometric, arrows}


\newcommand*{\progName}[1]{\mbox{\texttt{#1}}}

\usepackage{geometry}
\geometry{
  paper=a4paper,
  margin=54pt,
  includeheadfoot
}

% \documentclass{amsart}

% \usepackage[margin=1in]{geometry}        
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath,amsfonts,amssymb}
% \usepackage{todonotes}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}
\usepackage{soul}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%package for references
\usepackage[backend=biber, style=alphabetic, sorting=anyt]{biblatex}
\usepackage{csquotes}

\addbibresource{sources.bib}


\title{Pacman Capture the Flag \\ Assignment 4 Report}

\author{\hspace*{-0.5cm}
GROUP 03\\
\begin{tabular}{cccc}
Shekhar Devm Upadhyay & Aiman Shenawa\\
20010531 & 20001021 \\
sdup@kth.se & ashenawa@kth.se \\
\includegraphics[width=0.2\linewidth]{side_profile.jpg} & 
\includegraphics[width=0.17\linewidth]{Aiman_picture.jpg}
\end{tabular}} 
\date{May 2023}

% \pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhf{}
\lhead{DD2438} % DO NOT REMOVE!!!!
\rhead{Shekhar Devm Upadhyay, Aiman Shenawa} %% UPDATE WITH YOUR NAMES

\addbibresource{sources.bib}

\begin{document}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
% Describe the problem and importance in detail
% why it is important to study

% introduce pacman capture the flag problem
This report presents a study on the development of AI agents for controlling a team of two in a competitive Pac-Man Capture the Flag (CTF) scenario. The objective of the agents is to maximize their score by efficiently collecting food from the enemy turf within a limited number of moves. This report studies various approaches used in the field of artificial intelligence, including minimax game tree search, modifications like alpha-beta pruning and move ordering, Monte Carlo Tree Search (MCTS), Expectimax, and decision-tree-based approaches. Then, it goes on to present the final solution employed by the authors, and does a comparative analysis of the approaches utilized by all the teams in the competition. By analyzing and comparing these strategies, we aim to identify effective techniques for AI-controlled agents in Pac-Man CTF.

\end{abstract}
\clearpage

%% REMEMBER TO WRITE IN A TOP-DOWN FASHION, STARTING EACH SECTION WITH A SUMMARY. 

\section{Introduction}
\label{sec:introduction}
% Describe the problem and importance in detail

With the continious development of society and technology, the need for autonomous agents is increasing. It is becoming more and more important to develop agents that can operate in dynamic and complex environments autonomously.
In this report we will explore the problem of developing an agent that can play the game of Pacman capture the flag. This is a project developed by UC Berkely \cite{UCBContest2015}, it is a challenge that combines classic video game mechanics with artificial intelligence techniques.
The goal of the project is to develop an agent that can play the game of Pacman capture the flag. The game is played on a grid, where the agent can move in four directions, up, down, left and right. 
The grid is split into two halves, in each half there is a team of Pacman agents, the goal of the game is to capture the food pellets from the other team's side and bring them back to your side while also preventing the other team from doing the same.
Multiple aspects of artificial intelligence will be explored in this report, such as minimax game tree search, Monte Carlo Tree Search (MCTS), Expectimax, and decision-tree-based approaches. The final solution included the use of Minimax and alpha-beta pruning.




% include new figure from folder figuresA4,
% figure showing the game environment
%insert figure
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{./figuresA4/pacman_illustration.png}
  \caption{Illustration of environment}
  \label{fig:drag_force}
\end{figure}

% description of figure
Figure \ref{fig:drag_force} shows the game environment. The maze structure can be changes by specifying random seeds. The blue and red dots represent the food pellets, the blue and red lines represent the walls. 
As seen in the figure above there are both Pacman and ghosts. The agent is a ghost when defending its own side. When crossing over to the enemy side to collect pellets it becomes a pacman.
An agent can go into enemy territory and collect pellets, but the score will only increase when the agent returns back to its own side. In the figure there are also white pellets, 
those are power pellets, when an agent collects a power pellet it can eat the enemy agents for a period of 40 moves. During that period the enemy ghost agents are regarded as scared.
When an agent is eaten it respawns at its own side, if the agent was scared before getting eaten it will no longer be scared after respawning.
The game ends when all but two food pellets have been eaten, or when the time limit of 1200 moves has been reached. The team with the highest score wins the game.
There is a limit on calculation time, 15 seconds for initialization and 1 second for each move. If the agent exceeds the time limit three times during a game the team will forfeit the game.

The information each agent has access to is the following:
\begin{itemize}
  \item The friendly agent's positions
  \item The total score
  \item The position of all the food pellets on the map, this is updated every time an agent moves
  \item The position of all the power pellets on the map, this is updated every time an agent moves
  \item The position of an enemy agent if it is within Manhattan distance of 5 from us or a friendly agent.
\end{itemize}






\subsection{Task Description}

This report will discuss the problem of developing an agent that can play the game of Pacman capture the flag. 
The aim is to develop two agents which are able to defend their side of the map and also collect food pellets from the enemy side. 
The agents will be able to move in four directions, up, down, left and right. It is also possible to stay in the same position and not move, although this is only sometimes strategically beneficial.

These problems and their solutions are addressed in further detail in the following section \ref{rel_work}.




\subsection{Contribution}
% Describe the contribution of the report in detail

In this report we will impement Minimax and alpha-beta pruning into the UC Berkeley Pacman Capture the Flag project. We will also explore the use of Monte Carlo Tree Search (MCTS), Expectimax, and decision-tree-based approaches.
We will also compare the results of our solution with the results of other methods used for the same problem.

An essential aspect of Pacman capture the flag is the ability to cooperate as a team and communicate with the other agent. 
Potential real world applications of this project are many, as the ability of intelligent agents to make strategic decisions in a dynamic environment has many applications. 
Applications include multi-robot system coordination, autonomous vehicles, and unmanned aerial vehicles.







\subsection{Outline}
This report is organized as follows. First we introduce the problem and its importance in Section \ref{sec:introduction}. Then we discuss and highlight the related work and required backround knowledge in order to understand the proposed solution in Section \ref{rel_work}. This is followed by a detailed description of the proposed solution in Section \ref{method}. Next, we present the experimental setup and the results in Section \ref{sec:experiments_and_results}.
We summarize the results and conclude the report in Section \ref{sec:conclusion}. Lastly, we present improvements that can be made to the suggested solution in Section \ref{subsec:Future work}.












\section{Related Work}
\label{rel_work}
% Describe the related work in detail

The success of AI agents in Pac-Man CTF heavily relies on the choice and implementation of appropriate strategies. This section provides an overview of the relevant research in this domain.

\subsection{Minimax Game Tree Search}
\label{subsec:minimax}
The problem of finding the optimal move in a game can be formulated as a search problem. The search space is a tree, where each node represents a state of the game, and each edge represents a possible move. The root node represents the current state of the game, and the leaf nodes represent the terminal states of the game. The goal is to find the optimal path from the root node to a leaf node. 

The minimax algorithm \cite{vonNeumann1944} (shown in Algorithm \ref{alg:minimax}) is a recursive algorithm that computes the optimal move for a player in a two-player game. The algorithm assumes that the opponent plays optimally, and it tries to minimize the maximum loss that can be incurred by the player. It is a classic approach in Game Theory, aiming to find the optimal move in a two-player, zero-sum game. By constructing a game tree and evaluating the utility of different game states, the algorithm enables agents to make informed decisions. 

Sometimes, however, it is not feasible to search the entire game tree, due to the large number of possible moves. To address this issue, the algorithm can be modified to limit the depth of the search tree. The algorithm can also be modified to incorporate heuristics that evaluate the utility of non-terminal game states. Variations of minimax, such as alpha-beta pruning, move ordering based on heuristics, iterative deepening search (IDS), and Repeated States Checking (RSC), have been proposed to improve its efficiency and effectiveness \cite{CAMPBELL200257}.

\begin{algorithm}
\caption{Minimax Algorithm}
\label{alg:minimax}
\begin{algorithmic}[1]
\Procedure{Minimax}{$node, depth, maximizingPlayer$}
\If{$depth = 0$ or $node$ is a terminal node}
\State \textbf{return} the heuristic value of $node$
\EndIf
\If{$maximizingPlayer$}
\State $value \gets -\infty$
\For{each child of $node$}
\State $value \gets \max(value, \text{Minimax}(child, depth-1, \text{False}))$
\EndFor
\State \textbf{return} $value$
\Else
\State $value \gets \infty$
\For{each child of $node$}
\State $value \gets \min(value, \text{Minimax}(child, depth-1, \text{True}))$
\EndFor
\State \textbf{return} $value$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Expectimax}
\label{subsec:expectimax}
Expectimax is a variant of the minimax algorithm that considers uncertain outcomes in games. It is commonly used in domains with probabilistic elements. In Pac-Man CTF, Expectimax can be employed to account for the ghost movement and uncertain states, enabling agents to make rational decisions under uncertainty \cite{RIVEST198777}.


\subsection{Monte Carlo Tree Search}
\label{subsec:mcts}
MCTS is a sampling-based search algorithm that has demonstrated remarkable success in various game-playing domains. By combining tree exploration with random rollouts, MCTS performs effective exploration and exploitation of the search space. This approach has been applied to Pac-Man CTF, where agents simulate multiple playouts to estimate the value of different actions and make informed decisions \cite{GELLY20111856}.


\subsection{Decision Tree Based Approaches}
\label{subsec:decision_tree}
Decision trees offer a structured representation of decision-making processes. By learning from historical data and constructing decision rules, decision-tree-based approaches provide an interpretable framework for agent behavior. These techniques have been explored in Pac-Man CTF, where agents use decision trees to guide their actions based on various game state features \cite{Hastie2009}.


\section{Method}
\label{method}
% Proposed method section explaining what you did in more detail

In this section, we will discuss the solution proposed by our team. The solution is based on the Minimax algorithm, with some modifications to account for the limitations of the UC Berkeley Pacman Capture the Flag project. We will also discuss the motivations behind our heuristic functions.

\subsection{Modified Minimax}
\label{subsec:modified_minimax}
% comment: reasonable seems is not good measurement? unnecessary word? 
Typical Minimax approaches require that the entire game state be observable to us. Firstly, the UC Berkeley challenge does not provide us with the entire game state - we can only see the positions of our team members, and nearby enemies. Secondly, even if we knew the positions of all the enemies, if we consider the movements of both agents of our team as a single action, then the branching factor of the game tree is too large to be explored in a reasonable amount of time. So, we decided to modify the typical Minimax approach as follows:

\begin{itemize}
  \item At each time step, we consider the possible actions of the agent we are controlling (as the maximizing player). If there is an enemy agent visible to us, we consider the possible actions of the enemy agent as well, by passing control to the enemy as a minimizing player. If not, we continue to consider the possible actions of our agent (by exploring further based on moves that we can make in the next time step as the maximizing player). % comment: this does not need to be in (-)
  \item We use a heuristic to evaluate the utility of a game state, instead of evaluating the utility of a terminal state. We compute heuristics until a certain depth of the game tree, and use iteratively deepening search to explore the game tree further if time permits.
  \item We use alpha-beta pruning to reduce the number of nodes that we need to explore in the minimax exploration of the game tree. To further speed up the search, we use move ordering based on heuristics to explore the most promising nodes first, and implement repeated states checking to avoid computing values of the same game state multiple times when working with deeper game trees.
\end{itemize}

\subsection{Heuristics}
\label{subsec:heuristics}

We started off by having separate classes for offensive and defensive agents, each with a different heuristic. Later, we merged the two classes into a single class, and used a attribute called \texttt{mode} to determine if the behavior should be defensive or offensive, based on current agent position, position of our teammate, positions of any visible enemies, and other relevant factors affecting the game state.

\subsection{Offensive Heuristics}
\label{subsec:offensive_heuristics}
When in ``offensive'' mode, our agent tries to cross over to the other side of the map as quickly as possible, then actively looks for food pellets to capture. We prioritize closer food (without disregarding the rest), de-value food inside dead-end paths (paths that can be choked off at a single point by the enemy) pick up power pills when possible and when enemy agents are closeby. As the food in our pocket increases, we have an increasing tendency to return to our side of the map to deposit the food. We also try to avoid enemy agents when possible and try to eat them when they are scared. Being chased by an unscared enemy also ``turns off'' the tendency to chase food, and we try to run away from the enemy instead. We also try to avoid getting trapped in dead-end paths, and try to avoid getting cornered by enemy agents. After depositing the food, we start this process again.

We noticed that the offensive agents sometimes got stuck in local extrema of the value function. For example, sometimes the agent sat next to a food particle and didn't eat it, until the enemy agent came closer and pushed it into scramble mode. To avoid this, we added an increasing tendency to return to our own side for each time step in which the chosen action was to stay in the same place. This helped the agent get out of local extrema in a couple of moves, in most cases.

\subsection{Defensive Heuristics}
\label{subsec:defensive_heuristics}

For the ``defense'' mode of the agent, we incentivize minimizing the sum of distances to all the food we are defending. Since food near the center line is easier to take by the enemy, we give more weight to being close to those food particles. We keep track of the food that is being defended by our agent, and if some of it disappears, we know that an enemy agent has eaten it. We try to chase the enemy agent based on direct sight, knowledge from our teammate, and disappearing food. If we are close enough to the enemy agent, we move to trap and eat it we are not scared. If we are scared, we try to run away from the enemy agent. In no circumstance does the agent go over to enemy territory when in defense mode.

The exact details of our implementation can be found in our Github Repository and submitted final code.

\section{Experiments and Results}
\label{sec:experiments_and_results}

First, we will describe the experimental setup for the Pacman CTF game, and then we will present the results of our experiments. We will also discuss the merits and shortcomings of our solution, and compare it with the solutions of other groups.

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

We ran our experiments on the UC Berkeley Pacman Capture the Flag project. We used the provided baseline agents as a starting point, and modified them to implement our solution. We used the provided \texttt{capture.py} script to run the experiments, modifying only the \texttt{myTeam} file to use our agents. We used the provided \texttt{capture\_agent.py} file as a starting point for our agents, and modified it to implement our solution as described in Section \ref{method}.

All teams agreed on specific map sizes, seeds, and number of agents per team for the pre-final and final tournaments. We also agreed on a maximum compute time for each move, to ensure that the games would finish in a reasonable amount of time. One of the drawbacks of this approach is that different machines may be able to process different amounts of the game tree in the given time, and hence the results may not be completely fair. However, we believe that this is a reasonable trade-off, since it is not possible to ensure that all machines are equally powerful. We did not explicitly penalize teams for exceeding the time limit, relying on the honor system instead. There was a similar limit on the amount of time that a team could take upon initialization, to ensure that the games would start in a reasonable amount of time. We ran into situations where some teams still took a much longer time to initialize, leading to delays in the start of the game. We have not explicitly penalized teams for this, but we believe that this is something that should be taken into account in future tournaments.

\subsubsection{Experimental setup for measurement of performance}
In order to evaluate the performance of the suggested solution, we will compare our approach with five other groups. 
This is done by putting the algorithms against each other in the simulation. 
Two different evaluations will be done, one called pre-finals, in which the groups give their initial solutions, and one called finals, in which after having time to re-evaluate their solutions, the groups give their final solutions.
The results of both evaluations are presented in the tables below.
Results are measured in terms of wins, ties, losses. The score is calculated as the following: 3 points for a win, 1 point for a tie, and 0 points for a loss. The total score is the sum of the points for each group.




\subsection{Results}
\label{subsec:results}
In this section, we summarize the results of our experiments. First we present the results of the two evaluations in the competition, then we present the approach of the winning group.

% table with 5 collumns and 4 rows for the following results:
% 1. Group no. , 2. wins, 3. ties 4. losses 5. total score
% G4, 0, 0, 0, 0
% G2 , 0, 0, 0, 0
% G6 , 0, 0, 0, 0
% G3 , 0, 0, 0, 0
% highlighting G3 

\begin{table}[!hptb]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Group} & \textbf{Wins} & \textbf{Ties} & \textbf{Losses} & \textbf{Total Score} \\
    \hline
    4 & 8 & 0 & 0 & 24 \\
    \hline
    2 & 5 & 1 & 0 & 16 \\
    \hline
    6 & 5 & 1 & 3 & 16 \\
    \hline
    3 & 3 & 5 & 2 & 14 \\
    \hline
    1 & 0 & 7 & 3 & 3 \\
    \hline
  \end{tabular}
  \caption{Results of the pre-finals evaluation.}
  \label{tab:results_prefinals}
\end{table}


% finals results:
% G4: 13 wins, 1 tie, 1 loss, 40 points
% G2: 9 wins, 2 ties, 4 losses, 29 points
% G6: 7 wins, 2 ties, 6 losses, 23 points
% G3: 5 wins, 4 ties, 6 losses, 19 points
\begin{table}[!hptb]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Group} & \textbf{Wins} & \textbf{Ties} & \textbf{Losses} & \textbf{Total Score} \\
    \hline
    4 & 13 & 1 & 1 & 40 \\
    \hline
    2 & 9 & 2 & 4 & 29 \\
    \hline
    6 & 7 & 2 & 6 & 23 \\
    \hline
    3 & 4 & 4 & 6 & 16 \\
    \hline
    1 & 0 & 13 & 2 & 2 \\
    \hline
  \end{tabular}
  \caption{Results of the finals evaluation.}
  \label{tab:results_finals}
\end{table}






As agreed upon between the groups, we ran the final games on maps  of size 32x32, with 2 agents on each team, with varying seeds determining the map layout. We ran one game against each group for each seed, and recorded the results in Table \ref{tab:results_finals}. In contrast, the pre-final tournament was run on maps of size 16x16. The results of the pre-final tournament are shown in Table \ref{tab:results_prefinals}. In both cases, to compute the final scores, we gave 3 points for a win, 1 point for a tie, and 0 points for a loss.


% table of pre-final results

% \begin{table}[!hptb]
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline
%     \textbf{Seed} & \textbf{Group 1} & \textbf{Group 2} & \textbf{Group 3} & \textbf{Group 4} & \textbf{Group 5} & \textbf{Group 6} \\
%     \hline
%     53 & tie, tie & tie, lose & NA & win, lose & win, tie & lose, lose \\
%     \hline
%     63 & tie, tie & tie, tie & NA & win, lose & tie, tie & lose, lose \\
%     \hline
%     % \textbf{Final Score} & 3 & 16 & 14 & 24 & 12 & 16 \\
%     \hline

%   \end{tabular}
%   \caption{Performance of our agents against other groups in the pre-final tournament}
%   \label{tab:results_pre_final}
% \end{table}

% table of final results

\begin{table}[!hptb]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \textbf{Seed} & \textbf{Group 1} & \textbf{Group 2} & \textbf{Group 3} & \textbf{Group 4} & \textbf{Group 5} & \textbf{Group 6} \\
    \hline
    31 & win & loss & NA & tie & loss & loss \\
    \hline
    89 & win & win & NA & tie & loss & win \\
    \hline
    489 & tie & tie & NA & loss & win & loss \\
    \hline
    \textbf{Final Score} & 2 & 29 & 16 & 40 & 17 & 23 \\
    \hline

  \end{tabular}
  \caption{Performance of our agents against other groups in the final tournament}
  \label{tab:results_final}
\end{table}

During the pre-final, our agents were reasonably good, but we still had some bugs that caused them to perform poorly in some situations. For example, sometimes our offensive agent thought it was better to sit next to a food pellet than to eat it. Sometimes, our defensive agent kept the enemy in sight without moving in for an obvious kill. At this point, our agents still had fixed roles (one attack and one defense), and did not switch roles based on the state of the game.

Between the pre-final and final tournaments, we fixed the bugs in our agents, and also implemented the role switching mechanism described in Section \ref{subsec:heuristics}. This allowed our agents to switch roles based on the state of the game, and also to switch roles if one of the agents was killed. This improved the performance of our agents significantly in our testing. However, we overlooked something quite important - the size of the map on which we tested our ``Fluid'' Agents was much smaller than the size of the map on which the final tournament was run. This meant that our agents were not able to switch roles as seamlessly, often leaving gaps in our defense. This was compounded by the fact that our agents were not able to see the entire map, and hence could not plan their actions accordingly. This led to our agents performing poorly in the final tournament. We believe that if we had tested our agents on larger maps and spend some more time tuning the parameters, we would have been able to perform much better in the final tournament.


From Tables \ref{tab:results_prefinals} and \ref{tab:results_finals}, it is clear that the overall winner of the tournament was Group 4, who had the highest scores on both the pre-final and final tournaments. 
There was an issue where they took about 4 minutes on most teams' machines to initialize their agents instead of the agreed 15 seconds, and could thus be disqualified, but we agreed to let them participate in the tournament anyway. Even on their own computers, their code took about 2 minutes to initialize; the reason was that they had tuned their algorithm to use about 15 seconds for a smaller (16 $\times$ 16) map, but a larger map meant that their algorithm took much longer to initialize.

Their approach relies heavily on a Monte-Carlo Tree Search, coupled with a heuristic function to evaluate the value of a game state instead of using rollouts to the end of the game. 
They also had a very good estimator that allowed them to ``guess'' where the enemy agents could be, and act accordingly. This allowed them to perform very well in the tournament.

Group 2 came in second place, with both the offensive and defensive behaviors being based on a simple Decision Tree based approach. They had one defensive agent and one offensive agent, with no role switching mechanism. They also estimated the current position of the enemy agents, based on an HMM model. This gave them good intelligence on the enemy agents, and allowed them to perform well in the tournament.

Group 6 also had an interesting approach - their defensive behavior was based on a simple Decision Tree based approach, while the offensive behavior used a minimax implementation quite like our own. They had a much better role-switching mechanism than ours: when they had a lead of more than 10 points, both their agents would switch to defense. If their defense was scared, they would switch to offense and focus on eating food pellets, since they could anyways not defend their own food pellets at that point. Similarly, if both enemy agents were scared, both their agents would switch to offense and try to eat the enemy food pellets. This allowed them to perform quite well in the tournament, and secured them the third place.

Group 1 had an approach based on Reinforcement Learning. Though it was expected that such approaches could be competitive, their performance in the tournament was not as good as expected. This could be because there was no way to train their agents on the actual tournament map, and they had to train their agents on a smaller map. There was also the problem of the agents not being able to see the entire map, which led to hurdles in the training pipeline. They concluded that RL was not a good approach for this problem.

\section{Summary and Conclusions}
\label{sec:conclusion}

The purpose of this project was to develop an agent capable of playing the game of Pacman capture the flag, utilizing various artificial intelligence techniques. 
We explored various methods, including minimax game tree search, Monte Carlo Tree Search (MCTS), Expectimax, and decision-tree-based approaches. 
The final solution included a modified Minimax, alpha-beta pruning. This, in combination with the heuristics, allowed our agents to perform reasonably well.
The modified Minimax was implemented by considering the possible actions of the agent we are controlling (as the maximizing player), and when our agent saw an enemy agent, we considered the possible actions of the enemy agent as well (as the minimizing player).
Further, we also implemented role switching between the agents, which allowed them to switch roles based on the state of the game, and also to switch roles if one of the agents was killed.

During the pre-final stage, the agents showed good performance, but we still had some bugs that caused them to perform poorly in some situations. 
A bug that affected our performance greatly was that sometimes our offensive agent thought it was better to sit next to a food pellet than to eat it.
This was dealt with by changing the heuristics and adidng a penalty for staying in the same place.

Between the pre-final and final tournaments, we fixed the bugs in our agents, and also implemented the role switching mechanism described in Section \ref{subsec:heuristics}.
The role switching mechanism performed well in the pre-final stage maps, however, in the finals the map size was increased and the agents were not able to switch roles as seamlessly, often leaving gaps in our defense.

Despite improvements made to the agents, the agents' performance suffered in the final tournament due to these challenges. It is believed that testings on larger maps and more time spent tuning the parameters would have improved the performance of the agents.

















\section{Future Work}
% \subsection{Improvements}
\label{subsec:Future work}
% In this section we will discuss the improvements that can be made to the suggested solution. 
In this section we will discuss the improvements that can be made to the suggested solution. 
These improvements are based on the observations made during the development of the solution and during the finals as well as changes the group would have made if more time was available.

% During the development of the solution, we noticed that the agents sometimes got stuck in local extrema of the value function. 
During the development of the solution, we observed that the offensive agents occasionally became trapped in local extrema of the value function. For instance, they would sit next to a food particle without consuming it until an enemy agent approached, forcing them into scramble mode.
To avoid this, we added an increasing tendency to return to our own side for each time step in which the chosen action was to stay in the same place. 
This helped the agent get out of local extrema in a couple of moves, in most cases. 
However this is not a perfect solution, as we risk getting stuck in a position within a chokehold in the enemy territory. This roots to the heuristic design we have chosen and in order to improve upon this a in depth analysis of the different heuristics is required.

In the finals we observed a peculiar behavior of the offensive agent; the enemy ghost was chasing our agent, but at 5 distances away, our agent moved in a loop following the exact same route multiple times. This suggests that the heuristics should include a time variant factor in order to avoid loops.

In the development of the role switching mechanism, we implemented it analysing the original map sizes from the pre-finals. 
As a result of this we observe that there were times when our field was unprotected and the enemy agents could easily pass through. 
This is when an offensive agent is killed and the role switching mechanism is triggered. At that point, the defensive agent becomes the offensive agent and starts advancing into the enemy territory. As a result of this behavior, the now defensive agent is offset from its optimal guarding position, as the agent was just respawned, and the result is our territory being unguarded. The improvement our group suggest for this is to not trigger the switch until the respawned agent has had the time to move towards the middle of our half plane.








% \clearpage
% \newpage
\printbibliography
% \newpage

% \appendix
% % print the appendix title on the top of the page
% \section{Appendix}
% \label{sec:appendix}








% \paragraph{Relevance of the Problem}

% table layout
% \begin{table}[!hptb]
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline
%     \textbf{Terrain} & \multicolumn{2}{|c|}{\textbf{Open Field}} & \multicolumn{2}{|c|}{\textbf{Intersection}} & \multicolumn{2}{|c|}{\textbf{Unstructured}} \\
%     \hline
%     \textbf{Group} & \textbf{Circle} & \textbf{Random} & \textbf{Circle} & \textbf{Random} & \textbf{Circle} & \textbf{Random} \\
%     \hline
%     9 & 29 & 28.52 & 44 & 46 & 47 & 40.78 \\
%     \hline
%     13 & 34 & 47 & 55 & 47 & 50 & 47 \\
%     \hline
%     3 & \color{blue}{20.5} & \color{blue}{21} & \color{red}{287} & \color{blue}{32} & \color{blue}{30.5} & \color{blue}{26.5} \\
%     \hline
%     12 & 27.06 & 27.54 & 88.66 & 166.03 & 47.7 & 69.23 \\
%     \hline
%     10 & 42.8 & 42.9 & 64.6 & 158.42 & 52.05 & 69.88 \\
%     \hline
%     16 & 59.4 & 38 & 141.6 & 191.5 & 77.2 & 70.5 \\
%     \hline
%     \textbf{14} & 31.3 & 29.68 & 79.9 & 260.15 & 52.4 & 60 \\
%     \hline
%     \textbf{Overall Best Time} & 20.5 & 21 & 44 & 32 & 30.5 & 26.5 \\
%     \hline
%   \end{tabular}
%   \caption{Completion Times of the top 7 groups with drones on different terrains.}
%   \label{tab:results_drone_CA}
% \end{table}


% % figure showing pseudo traffic lights
% \begin{figure}[!hptb]
%   \centering
%   \includegraphics[width= 0.6\textwidth]{./figures/pseudo_traffic_lights.png}
%   \caption{Pseudo-traffic lights. Note that the repulsion force is only along the direction of the path. So the red car slows down a lot, while the blue car does not decelerate as much. The blue car can pass the intersection before the red car, without steering away from its original path.}
%   \label{fig:pseudo_traffic_lights}
% \end{figure}


% % figure showing ball chasing
% \begin{figure}[!hptb]
%   \centering
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{./figures/soccer_VO_attraction_setup.png}
%     \caption{VO\_Attraction Setting}
%     \label{fig:soccer_VO_attraction_setting}
%   \end{subfigure}
%   ~
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{./figures/soccer_VO_attraction_setup_2.png}
%     \caption{VO\_Attraction Initial Setup}
%     \label{fig:soccer_VO_attraction_initial_setup}
%   \end{subfigure}
%   \caption{Setup for our Ball-chasing strategy.}
% \end{figure}



% table comparing results of different groups (drones)

% \begin{table}[!hptb]
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline
%     \textbf{Terrain} & \multicolumn{2}{|c|}{\textbf{Open Field}} & \multicolumn{2}{|c|}{\textbf{Intersection}} & \multicolumn{2}{|c|}{\textbf{Unstructured}} \\
%     \hline
%     \textbf{Group} & \textbf{Circle} & \textbf{Random} & \textbf{Circle} & \textbf{Random} & \textbf{Circle} & \textbf{Random} \\
%     \hline
%     9 & 29 & 28.52 & 44 & 46 & 47 & 40.78 \\
%     \hline
%     13 & 34 & 47 & 55 & 47 & 50 & 47 \\
%     \hline
%     3 & \color{blue}{20.5} & \color{blue}{21} & \color{red}{287} & \color{blue}{32} & \color{blue}{30.5} & \color{blue}{26.5} \\
%     \hline
%     12 & 27.06 & 27.54 & 88.66 & 166.03 & 47.7 & 69.23 \\
%     \hline
%     10 & 42.8 & 42.9 & 64.6 & 158.42 & 52.05 & 69.88 \\
%     \hline
%     16 & 59.4 & 38 & 141.6 & 191.5 & 77.2 & 70.5 \\
%     \hline
%     \textbf{14} & 31.3 & 29.68 & 79.9 & 260.15 & 52.4 & 60 \\
%     \hline
%     \textbf{Overall Best Time} & 20.5 & 21 & 44 & 32 & 30.5 & 26.5 \\
%     \hline
%   \end{tabular}
%   \caption{Completion Times of the top 7 groups with drones on different terrains.}
%   \label{tab:results_drone_CA}
% \end{table}




\end{document}
